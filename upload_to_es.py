import os
import pandas as pd
from elasticsearch import helpers
from utils import get_es_client

def load_csv_data(csv_path):
    """Load incident tickets data from CSV file."""
    df = pd.read_csv(csv_path)
    print(f"ðŸ“‚ Loaded {len(df)} incident tickets from CSV")
    return df

def prepare_data_for_es(df):
    """Prepare data for Elasticsearch indexing."""
    df = df.copy()
    
    # Convert date columns to proper datetime format (if they're not already)
    date_columns = ['Opened Date', 'Ticket Resolved Date', 'Ticket Closed Date', 'Target Finish Date']
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
    
    # Convert numeric columns to proper types
    numeric_columns = [
        'Ticket Priority', 'Time to resolve (min)', 'Resolution Time (hours)', 
        'SLA Breach', 'Opened Hour', 'Opened Month', 'Opened Quarter', 
        'First Response Time (min)'
    ]
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Convert boolean/categorical columns to lowercase strings for consistency
    categorical_columns = [
        'Ticket Class', 'Ticket Status', 'Ticket Status Original', 
        'Executed Automata', 'Recommended Automata', 'Actionable', 
        'Autogenerated', 'Automation Engine', 'Automation Success',
        'Escalation Level', 'Impact Level'
    ]
    for col in categorical_columns:
        if col in df.columns:
            df[col] = df[col].astype(str).str.lower()
    
    print(f"âœ… Data prepared for Elasticsearch")
    return df

def index_to_elastic(df, es, index):
    """Index incident tickets to Elasticsearch."""
    import numpy as np
    import json
    
    # Make a copy to avoid modifying the original DataFrame
    df = df.copy()
    
    # Convert datetime columns to ISO format strings for ES
    for col in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            # Handle NaT (Not a Time) values properly
            df[col] = df[col].apply(lambda x: x.strftime('%Y-%m-%dT%H:%M:%S') if pd.notna(x) else None)
    
    # Replace all NaN/inf values with None BEFORE converting to dict
    df = df.replace([np.nan, np.inf, -np.inf], None)
    
    # Convert to records and prepare for bulk indexing
    actions = []
    success_count = 0
    error_count = 0
    
    for i, record in enumerate(df.to_dict('records')):
        # Use Ticket Number as ID if available
        doc_id = record.get('Ticket Number', f'incident_{i}')
        
        # Clean the record: replace any remaining NaN/None-like values
        cleaned_record = {}
        for key, value in record.items():
            # Check if value is NaN using different methods
            if value is None:
                cleaned_record[key] = None
            elif isinstance(value, float):
                if np.isnan(value) or np.isinf(value):
                    cleaned_record[key] = None
                else:
                    cleaned_record[key] = value
            elif isinstance(value, str) and value.lower() in ['nan', 'null', 'none']:
                cleaned_record[key] = None
            else:
                cleaned_record[key] = value
        
        actions.append({
            "_index": index,
            "_id": doc_id,
            "_source": cleaned_record
        })
    
    # Bulk index with error handling
    for ok, result in helpers.streaming_bulk(es, actions, raise_on_error=False):
        if ok:
            success_count += 1
        else:
            error_count += 1
            # Print first few errors for debugging
            if error_count <= 3:
                print(f"âŒ Error: {result}")
    
    es.indices.refresh(index=index)
    
    if error_count > 0:
        print(f"âš ï¸  Indexed {success_count} records to Elasticsearch ({error_count} had errors)")
    else:
        print(f"âœ… Indexed {success_count} records to Elasticsearch")

def delete_index(es, index):
    """Delete an Elasticsearch index."""
    try:
        if es.indices.exists(index=index):
            es.indices.delete(index=index)
            print(f"Index '{index}' deleted successfully")
            return True
        else:
            print(f"â„¹ Index '{index}' does not exist")
            return False
    except Exception as e:
        print(f"Error deleting index: {e}")
        return False

def debug_index(es, index):
    """Debug and display index information."""
    print(f"Checking index: {index}\n")
    
    try:
        # Get a sample document
        response = es.search(index=index, size=1)
        
        if response['hits']['total']['value'] > 0:
            doc = response['hits']['hits'][0]['_source']
            print(f"Total documents in index: {response['hits']['total']['value']}")
            print(f"\nFields in Elasticsearch document ({len(doc)} fields):\n")
            
            for i, field in enumerate(sorted(doc.keys()), 1):
                value = doc[field]
                value_type = type(value).__name__
                value_preview = str(value)[:50] if value else "null"
                print(f"{i}. {field} ({value_type}): {value_preview}")
            
            # Get mapping
            mapping = es.indices.get_mapping(index=index)
            properties = mapping[index]['mappings'].get('properties', {})
            
            print(f"\n\nElasticsearch Mapping ({len(properties)} mapped fields):\n")
            for i, (field, config) in enumerate(sorted(properties.items()), 1):
                field_type = config.get('type', 'object')
                print(f"{i}. {field}: {field_type}")
        else:
            print("No documents found in index!")
    except Exception as e:
        print(f"Error: {e}")

def main():
    # Get ES client
    es = get_es_client()
    index = os.getenv("ES_INDEX", "incident_tickets_uploaded")
    
    # Path to cleaned and enriched CSV file (already cleaned and enriched by dataClean.py)
    csv_path = os.path.join(os.path.dirname(__file__), 'cleaned_enriched_incident_tickets_updated_for_visualization.csv')
    
    # Load CSV data
    df = load_csv_data(csv_path)
    
    # Prepare data for Elasticsearch (minimal processing since data is already cleaned and enriched)
    df = prepare_data_for_es(df)
    
    # Index to Elasticsearch
    index_to_elastic(df, es, index)
    
    print(f"\nðŸŽ‰ Upload complete! {len(df)} incident tickets indexed to {index}")

if __name__ == "__main__":
    main()
